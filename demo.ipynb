{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing Flappy Bird with RL\n",
    "\n",
    "**Tom Labiausse** - March 2024\n",
    "\n",
    "[Code repository associated to the projet](https://github.com/t0m1ab/flappy-bird)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook is organized in 5 main parts:\n",
    "* **0 - Librairies and global variables**\n",
    "* **1 - Agents**\n",
    "* **2 - Trainers**\n",
    "* **3 - Utility functions** [define plot functions]\n",
    "* **4 - Launch and visualize** [launch trainings and plot/compare results]\n",
    "\n",
    "During the training of an agent, data is recorded and save in JSON files at the end for future visualization as well as the agents final q-values. In section 0, `DEFAULT_OUTPUTS_PATH` and `DEFAULT_MODELS_PATH` define the default location to save the training data and the agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Librairies and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import json\n",
    "import time\n",
    "from abc import abstractmethod\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator\n",
    "import gymnasium as gym\n",
    "import text_flappy_bird_gym\n",
    "\n",
    "DEFAULT_OUTPUTS_PATH = os.path.join(os.getcwd(), \"outputs/\")\n",
    "DEFAULT_MODELS_PATH = os.path.join(os.getcwd(), \"models/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCAgent is ready!\n",
      "SARSALambdaAgent is ready!\n"
     ]
    }
   ],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_space_size: int,\n",
    "        discount_factor: float,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize a Reinforcement Learning agent with an empty dictionary of state-action values (q_values), a learning rate and an epsilon.\n",
    "\n",
    "        ARGUMENTS:\n",
    "            - action_space_size: The number of possible actions\n",
    "            - discount_factor: The discount factor for computing the Q-value\n",
    "            - q_values: A dictionary of state: [action values]\n",
    "        \"\"\"\n",
    "        self.action_space_size = int(action_space_size)\n",
    "        self.discount_factor = discount_factor # gamma\n",
    "        self.q_values = defaultdict(lambda: [0 for _ in range(self.action_space_size)]) # default q_value for a given state is given by np.zeros(2) = [0, 0]\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return self.__class__.__name__\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_state(str_state: str) -> tuple[int, int]:\n",
    "        \"\"\" str: \"x,y\" -> tuple: (x,y) \"\"\"\n",
    "        elements = str_state.split(\",\")\n",
    "        if not len(elements) == 2:\n",
    "            raise ValueError(f\"Invalid state string: '{str_state}'. Should look like 'x,y'\")\n",
    "        return (int(elements[0]), int(elements[1]))\n",
    "\n",
    "    @staticmethod\n",
    "    def from_pretrained(agent_filename: str, path: str = None) -> \"MCAgent | SARSALambdaAgent\":\n",
    "        \"\"\"\n",
    "        Load the agent from a json file.\n",
    "        \"\"\"\n",
    "        path = path if path is not None else DEFAULT_MODELS_PATH\n",
    "        filepath = os.path.join(path, f\"{agent_filename}.json\")\n",
    "        if not os.path.isfile(filepath):\n",
    "            raise FileNotFoundError(f\"File {filepath} not found\")\n",
    "        \n",
    "        with open(filepath, \"r\") as f:\n",
    "            agent_dict = json.load(f)\n",
    "\n",
    "        agent_constructor = None\n",
    "        if agent_dict[\"type\"] == \"MCAgent\":\n",
    "            agent_constructor = MCAgent\n",
    "        elif agent_dict[\"type\"] == \"SARSALambdaAgent\":\n",
    "            agent_constructor = SARSALambdaAgent\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid agent type {agent_dict['type']}\")\n",
    "\n",
    "        agent = agent_constructor(action_space_size=agent_dict[\"action_space_size\"])\n",
    "\n",
    "        agent.q_values = defaultdict(lambda: [0 for _ in range(agent.action_space_size)])\n",
    "        for k, v in agent_dict[\"q_values\"].items():\n",
    "            agent.q_values[Agent.parse_state(k)] = np.array(v)\n",
    "\n",
    "        return agent\n",
    "\n",
    "    def save(self, name: str = None, path: str = None) -> None:\n",
    "        \"\"\"\n",
    "        Save the agent in a json file.\n",
    "        \"\"\"\n",
    "        name = name if name is not None else self.__class__.__name__\n",
    "        json_filename = f\"{name}.json\"\n",
    "        path = path if path is not None else DEFAULT_MODELS_PATH\n",
    "        Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        serialized_q_values = {\n",
    "            f\"{k[0]},{k[1]}\": list(v) for k, v in self.q_values.items()\n",
    "        }\n",
    "\n",
    "        json_dict = {\n",
    "            \"type\": self.__class__.__name__,\n",
    "            \"action_space_size\": self.action_space_size,\n",
    "            \"q_values\": serialized_q_values,\n",
    "        }\n",
    "\n",
    "        with open(os.path.join(path, json_filename), \"wb\") as f:\n",
    "            f.write(json.dumps(json_dict).encode(\"utf-8\"))\n",
    "\n",
    "        print(f\"Agent {self} saved in {os.path.join(path, json_filename)}\")\n",
    "    \n",
    "    @abstractmethod\n",
    "    def policy(self, state: tuple[int, int], env: gym.Env = None, epsilon: float = None) -> int:\n",
    "        \"\"\"\n",
    "\t\tReturns an action following an epsilon-soft policy. If env is None and epsilon is None, the agent acts greedily (inference mode).\n",
    "\t\t\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def update(self, *args, **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Updates the Q-value of an action following a specific method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class MCAgent(Agent):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            action_space_size: int, \n",
    "            discount_factor: float = None,\n",
    "            lr: float = None,\n",
    "            verbose: bool = False,\n",
    "        ):\n",
    "        super().__init__(action_space_size=action_space_size, discount_factor=discount_factor)\n",
    "        self.lr = lr\n",
    "        self.mean_return = defaultdict(lambda: (0,0)) # returns[x] is (0,0) by default for any x and represents (n, R_n) [see S&B section 2.4]\n",
    "        self.verbose = verbose\n",
    "        if self.verbose:\n",
    "            if self.lr is not None:\n",
    "                print(f\"Using learning rate {self.lr} update\")\n",
    "            else:\n",
    "                print(f\"Using mean return update\")\n",
    "    \n",
    "    def policy(self, state: tuple[int, int], env: gym.Env = None, epsilon: float = None) -> int:\n",
    "        \"\"\"\n",
    "        Returns an action following an epsilon-soft policy. If env is None and epsilon is None, the agent acts greedily (inference mode).\n",
    "        \"\"\"\n",
    "        if env is None and epsilon is None: # act greedily (inference mode)\n",
    "            return int(np.argmax(self.q_values[state]))\n",
    "        \n",
    "        if np.random.random() < epsilon: # with probability epsilon return a random action to explore the environment\n",
    "            return env.action_space.sample()\n",
    "        else: # with probability (1 - epsilon) act greedily (exploit)       \n",
    "            return int(np.argmax(self.q_values[state]))\n",
    "    \n",
    "    def update_mean_return(self, state: tuple[int, int], action: int, return_value: float) -> None:\n",
    "        \"\"\" \n",
    "        Update the mean return of the state-action pair (state, action) following the incremental mean formula [see S&B section 2.4].\n",
    "        \"\"\"\n",
    "        (n, R_n) = self.mean_return[(state, action)]\n",
    "        self.mean_return[(state, action)] = (n+1, (n * R_n + return_value) / (n+1))\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        states: list[tuple[int, int]],\n",
    "        actions: list[int],\n",
    "        rewards: list[float],\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Updates the Q-value of an action following the Monte-Carlo Exploring Starts method [see S&B section 5.3].\n",
    "        \"\"\"\n",
    "        state_action_pairs = list(zip(states, actions)) # need to transform into a list because zip is an iterator and will be consumed by the first for loop\n",
    "        T = len(states)\n",
    "        G = 0\n",
    "\n",
    "        for t in range(T-1,-1,-1): # loop over the state-action pairs in reverse order (from T-1 to 0)\n",
    "            G = self.discount_factor * G + rewards[t+1]\n",
    "            if not state_action_pairs[t] in state_action_pairs[:t]: # first visit of the (state, action) pair in the episode\n",
    "                state_t, action_t = state_action_pairs[t]\n",
    "                if self.lr is None: # use mean return update\n",
    "                    self.update_mean_return(state_t, action_t, return_value=G)\n",
    "                    self.q_values[state_t][action_t] = self.mean_return[state_action_pairs[t]][1]\n",
    "                else: # use learning rate update\n",
    "                    self.q_values[state_t][action_t] += self.lr * (G - self.q_values[state_t][action_t])\n",
    "\n",
    "\n",
    "class SARSALambdaAgent(Agent):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            action_space_size: int, \n",
    "            discount_factor: float = None,\n",
    "            lr: float = None,\n",
    "            trace_decay: float = None,\n",
    "        ):\n",
    "        super().__init__(action_space_size=action_space_size, discount_factor=discount_factor)\n",
    "        self.eligibility = defaultdict(lambda: [0 for _ in range(self.action_space_size)]) # eligibility[(s,a)] is 0 by default for any pair (state,action)\n",
    "        self.lr = lr\n",
    "        self.trace_decay = trace_decay\n",
    "\n",
    "    def policy(self, state: tuple[int, int], env: gym.Env = None, epsilon: float = None) -> int:\n",
    "        \"\"\"\n",
    "        Returns an action following an epsilon-soft policy. If env is None and epsilon is None, the agent acts greedily (inference mode).\n",
    "        \"\"\"\n",
    "        if env is None and epsilon is None: # act greedily (inference mode)\n",
    "            return int(np.argmax(self.q_values[state]))\n",
    "        \n",
    "        if np.random.random() < epsilon: # with probability epsilon return a random action to explore the environment\n",
    "            return env.action_space.sample()\n",
    "        else: # with probability (1 - epsilon) act greedily (exploit)       \n",
    "            return int(np.argmax(self.q_values[state]))\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        state: tuple[int, int, bool],\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        next_state: tuple[int, int, bool],\n",
    "        next_action: int,\n",
    "        terminated: bool,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Updates the Q-value of an action following the SARSA-lambda method [see S&B section 12.7].\n",
    "        \"\"\"\n",
    "        q_value = self.q_values[state][action]\n",
    "        next_q_value = (not terminated) * self.q_values[next_state][next_action]\n",
    "        td_error = reward + self.discount_factor * next_q_value - q_value\n",
    "\n",
    "        # sarsa-lambda\n",
    "        self.eligibility[state][action] += 1\n",
    "        for s in self.q_values.keys():\n",
    "            for a in range(self.action_space_size):\n",
    "                self.q_values[s][a] += self.lr * td_error * self.eligibility[s][a]\n",
    "                self.eligibility[s][a] *= (self.discount_factor * self.trace_decay)\n",
    "\n",
    "        # sarsa\n",
    "        # self.q_values[state][action] = q_value + self.lr * td_error\n",
    "\n",
    "\n",
    "# tests\n",
    "\n",
    "env = gym.make('TextFlappyBird-v0', height = 15, width = 20, pipe_gap = 4)\n",
    "\n",
    "mc_agent = MCAgent(\n",
    "\taction_space_size=env.action_space.n,\n",
    "\tdiscount_factor=1.0,\n",
    "    lr=0.01,\n",
    ")\n",
    "print(f\"{mc_agent} is ready!\")\n",
    "\n",
    "sarsa_lambda_agent = SARSALambdaAgent(\n",
    "\taction_space_size=env.action_space.n,\n",
    "\tdiscount_factor=0.95,\n",
    "\tlr=0.01,\n",
    "\ttrace_decay=0.9,\n",
    ")\n",
    "print(f\"{sarsa_lambda_agent} is ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCTrainer is ready!\n",
      "SARSALambdaTrainer is ready!\n"
     ]
    }
   ],
   "source": [
    "class Trainer():\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.env = None\n",
    "        self.agent = None\n",
    "        self.n_episodes = None\n",
    "        self.final_epsilon = None\n",
    "        self.discount_factor = None\n",
    "        self.experiment_name = None\n",
    "        self.max_episode_length_eval = None\n",
    "        self.reset_stats()\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return self.__class__.__name__\n",
    "\n",
    "    def reset_stats(self) -> None:\n",
    "        self.train_episode_durations = [] # list of train episode durations\n",
    "        self.train_episode_indexes = [] # list of train episode indexes matching eavh evaluation phase\n",
    "        self.eval_episode_durations = [] # list of lists of episode durations for each evaluation phase\n",
    "      \n",
    "    def eval(self, n_episodes: int, env: gym.Env = None, agent: Agent = None, verbose: bool = False) -> list[int]:\n",
    "        \"\"\"\n",
    "        Evaluate the agent with n_episodes in the environment by taking greedy actions and returns the lenghts of these eval episodes.\n",
    "        \"\"\"\n",
    "\n",
    "        env = self.env if env is None else env\n",
    "        agent = self.agent if agent is None else agent\n",
    "\n",
    "        pbar = range(n_episodes)\n",
    "        if verbose:\n",
    "            pbar = tqdm(pbar, desc=f\"Eval {self} on {self.env.spec.id}\")\n",
    "\n",
    "        episode_lengths = []\n",
    "        for _ in pbar:\n",
    "            \n",
    "            episode_lengths.append(0)\n",
    "            obs, _ = env.reset()\n",
    "            terminated = False\n",
    "            while not terminated:\n",
    "                action = agent.policy(obs) # greedy action\n",
    "                obs, _, terminated, _, _ = env.step(action)\n",
    "                episode_lengths[-1] += 1\n",
    "                if self.max_episode_length_eval is not None and episode_lengths[-1] >= self.max_episode_length_eval:\n",
    "                    break\n",
    "\n",
    "        return episode_lengths\n",
    "    \n",
    "    def save_eval_episode_durations_plot(self, path: str = None) -> None:\n",
    "        \"\"\" Save plot of evaluation episode durations. \"\"\"\n",
    "\n",
    "        if len(self.train_episode_indexes) == 0 or len(self.eval_episode_durations) == 0:\n",
    "            print(\"No evaluation episode durations to plot...\")\n",
    "            return\n",
    "        \n",
    "        path = DEFAULT_OUTPUTS_PATH if path is None else path\n",
    "        Path(path).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # curve\n",
    "        plt.plot(self.train_episode_indexes, [np.mean(lengths) for lengths in self.eval_episode_durations])\n",
    "\n",
    "        # config parameters\n",
    "        for k, v in self.get_config_dict().items():\n",
    "            plt.scatter([], [], label=f\"{k} = {v}\", color=\"white\")\n",
    "\n",
    "        # axis, title and legend\n",
    "        plt.xlabel(\"Training episode index\")\n",
    "        plt.ylabel(f\"Averaged episode duration (limit={self.max_episode_length_eval})\")\n",
    "        plt.title(f\"Evaluation episode duration over training\")\n",
    "        plt.legend(loc=\"lower right\", handletextpad=0, handlelength=0, fontsize=8)\n",
    "\n",
    "        # save plot\n",
    "        plt.savefig(os.path.join(path, f\"{self.experiment_name}_eval_durations.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # save data in json\n",
    "        with open(os.path.join(path, f\"{self.experiment_name}_eval_durations.json\"), \"w\") as f:\n",
    "            f.write(json.dumps({\n",
    "                \"experiment_name\": self.experiment_name,\n",
    "                \"max_episode_length_eval\": self.max_episode_length_eval,\n",
    "                \"train_episode_indexes\": self.train_episode_indexes,\n",
    "                \"eval_episode_durations\": self.eval_episode_durations,\n",
    "            }))\n",
    "    \n",
    "    def save_train_episode_durations_plot(self, path: str = None, window: float = None) -> None:\n",
    "        \"\"\" Save plot of training episode durations avegared on slots of window size. \"\"\"\n",
    "\n",
    "        if len(self.train_episode_durations) == 0:\n",
    "            print(\"No training episode durations to plot...\")\n",
    "            return\n",
    "        \n",
    "        if window is not None and window > len(self.train_episode_durations):\n",
    "            print(\"Window size should be smaller than the number of training episodes...\")\n",
    "            return\n",
    "        \n",
    "        path = DEFAULT_OUTPUTS_PATH if path is None else path\n",
    "        Path(path).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        indexes = np.arange(1, len(self.train_episode_durations) + 1)\n",
    "        if window is None: # cumulative average\n",
    "            avg_episode_durations = np.cumsum(self.train_episode_durations) / indexes\n",
    "            title_tag = \"(cumulative average)\"\n",
    "        else: # moving average\n",
    "            incomplete_avg = np.cumsum(self.train_episode_durations[:window-1]) / np.arange(1, window)\n",
    "            complete_avg = np.convolve(self.train_episode_durations, np.ones(window), 'valid') / window\n",
    "            avg_episode_durations = np.hstack((incomplete_avg, complete_avg))\n",
    "            assert len(avg_episode_durations) == len(self.train_episode_durations)\n",
    "            title_tag = f\"(average window = {window})\"\n",
    "\n",
    "        # curve\n",
    "        plt.plot(indexes, avg_episode_durations)\n",
    "\n",
    "        # config parameters\n",
    "        for k, v in self.get_config_dict().items():\n",
    "            plt.scatter([], [], label=f\"{k} = {v}\", color=\"white\")\n",
    "\n",
    "        # axis, title and legend\n",
    "        plt.xlabel(\"Training episode index\")\n",
    "        plt.ylabel(f\"Averaged episode duration\")\n",
    "        plt.title(f\"Train episode duration over training {title_tag}\")\n",
    "        plt.legend(loc=\"lower right\", handletextpad=0, handlelength=0, fontsize=8)\n",
    "\n",
    "        # save plot\n",
    "        plt.savefig(os.path.join(path, f\"{self.experiment_name}_train_durations.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # save data in json\n",
    "        with open(os.path.join(path, f\"{self.experiment_name}_train_durations.json\"), \"w\") as f:\n",
    "            f.write(json.dumps({\n",
    "                \"experiment_name\": self.experiment_name,\n",
    "                \"train_episode_durations\": self.train_episode_durations,\n",
    "            }))\n",
    "\n",
    "\n",
    "class MCTrainer(Trainer):\n",
    "\n",
    "    DEFAULT_EXP_NAME = \"mc\"\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            n_episodes: int, \n",
    "            discount_factor: float, \n",
    "            final_epsilon: float,\n",
    "            learning_rate: float = None,\n",
    "            n_eval: int = None, \n",
    "            max_episode_length_eval: int = None,\n",
    "        ) -> None:\n",
    "        super().__init__()\n",
    "        self.n_episodes = n_episodes\n",
    "        self.discount_factor = discount_factor\n",
    "        self.final_epsilon = final_epsilon\n",
    "        self.lr = learning_rate\n",
    "        self.n_eval = n_eval\n",
    "        self.max_episode_length_eval = max_episode_length_eval\n",
    "    \n",
    "    def get_config_dict(self) -> dict:\n",
    "        return {\n",
    "            \"n_episodes\": self.n_episodes,\n",
    "            \"discount_factor\": self.discount_factor,\n",
    "            \"final_epsilon\": self.final_epsilon,\n",
    "        }\n",
    "\n",
    "    def train(self, env: gym.Env, experiment_name: str = None, save_plots: bool = False, save_agent: bool = False) -> MCAgent:\n",
    "        \n",
    "        # self.env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=self.n_episodes)\n",
    "        self.env = env\n",
    "\n",
    "        self.agent = MCAgent(\n",
    "            action_space_size=self.env.action_space.n,\n",
    "            discount_factor=self.discount_factor,\n",
    "            lr=self.lr,\n",
    "        )\n",
    "\n",
    "        self.experiment_name = experiment_name if experiment_name is not None else MCTrainer.DEFAULT_EXP_NAME\n",
    "        \n",
    "        self.reset_stats()\n",
    "        eval_every_episode = self.n_episodes // self.n_eval if self.n_eval is not None else None\n",
    "        eval_every_episode = None if eval_every_episode == 0 else eval_every_episode\n",
    "\n",
    "        log_final_eps = np.log(self.final_epsilon)\n",
    "        pbar = tqdm(range(self.n_episodes), desc=f\"Train {self} on {self.env.spec.id}\")\n",
    "        for episode_idx in pbar:\n",
    "\n",
    "            epsilon = np.exp(log_final_eps * episode_idx / self.n_episodes)\n",
    "\n",
    "            obs, _ = self.env.reset()\n",
    "            # action = np.random.choice([0,1]) # random action to explore the environment\n",
    "            action = self.agent.policy(obs, env=self.env, epsilon=epsilon)\n",
    "            next_obs, reward, terminated, _, _ = self.env.step(action)\n",
    "\n",
    "            # define lists of S_t, A_t, R_t values [see S&B section 5.3]\n",
    "            states = [obs, next_obs] # S list\n",
    "            actions = [action] # A list\n",
    "            rewards = [None, reward] # R list\n",
    "            episode_length = 1\n",
    "\n",
    "            while not terminated:\n",
    "\n",
    "                action = self.agent.policy(states[-1], env=self.env, epsilon=epsilon)\n",
    "                next_obs, reward, terminated, _, _ = self.env.step(action)\n",
    "\n",
    "                states.append(next_obs)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                episode_length += 1\n",
    "            \n",
    "            if terminated: # the last state is probably out of the state space (because the chain terminated) so we don't need it\n",
    "                states.pop()\n",
    "\n",
    "            # at this stage states and actions should have T elements and rewards T+1 elements (uncomment the following lines to check)\n",
    "            assert len(actions) == len(states)\n",
    "            assert len(rewards) == len(states) + 1\n",
    "\n",
    "            # update the agent\n",
    "            self.agent.update(states, actions, rewards)\n",
    "            self.train_episode_durations.append(episode_length) # store the train episode duration\n",
    "\n",
    "            if eval_every_episode is not None and episode_idx % eval_every_episode == 0:\n",
    "                self.train_episode_indexes.append(episode_idx)\n",
    "                self.eval_episode_durations.append(self.eval(n_episodes=100))\n",
    "                pbar.set_postfix({f\"avg_eval_episode_duration\": np.mean(self.eval_episode_durations[-1])})\n",
    "                \n",
    "        if save_plots:\n",
    "            self.save_train_episode_durations_plot(window=100)\n",
    "            self.save_eval_episode_durations_plot()\n",
    "\n",
    "        if save_agent:\n",
    "            self.agent.save()\n",
    "        \n",
    "        return self.agent\n",
    "\n",
    "\n",
    "class SARSALambdaTrainer(Trainer):\n",
    "\n",
    "    DEFAULT_EXP_NAME = \"sarsa-lambda\"\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            n_episodes: int, \n",
    "            learnind_rate: float,\n",
    "            trace_decay: float,\n",
    "            discount_factor: float, \n",
    "            final_epsilon: float, \n",
    "            n_eval: int = None, \n",
    "            max_episode_length_eval: int = None\n",
    "        ) -> None:\n",
    "        super().__init__()\n",
    "        self.n_episodes = n_episodes\n",
    "        self.lr = learnind_rate\n",
    "        self.trace_decay = trace_decay\n",
    "        self.discount_factor = discount_factor\n",
    "        self.final_epsilon = final_epsilon\n",
    "        self.n_eval = n_eval\n",
    "        self.max_episode_length_eval = max_episode_length_eval\n",
    "    \n",
    "    def get_config_dict(self) -> dict:\n",
    "        return {\n",
    "            \"n_episodes\": self.n_episodes,\n",
    "            \"discount_factor\": self.discount_factor,\n",
    "            \"final_epsilon\": self.final_epsilon,\n",
    "            \"learning_rate\": self.lr,\n",
    "            \"trace_decay\": self.trace_decay,\n",
    "        }\n",
    "    \n",
    "    def train(self, env: gym.Env, experiment_name: str = None, save_plots: bool = False, save_agent: bool = False) -> SARSALambdaAgent:\n",
    "        \n",
    "        self.env = env\n",
    "\n",
    "        self.agent = SARSALambdaAgent(\n",
    "            action_space_size=self.env.action_space.n,\n",
    "            discount_factor=self.discount_factor,\n",
    "            lr=self.lr,\n",
    "            trace_decay=self.trace_decay,\n",
    "        )\n",
    "\n",
    "        self.experiment_name = experiment_name if experiment_name is not None else SARSALambdaTrainer.DEFAULT_EXP_NAME\n",
    "        \n",
    "        self.reset_stats()\n",
    "        eval_every_episode = self.n_episodes // self.n_eval if self.n_eval is not None else None\n",
    "        eval_every_episode = None if eval_every_episode == 0 else eval_every_episode\n",
    "\n",
    "        log_final_eps = np.log(self.final_epsilon)\n",
    "        pbar = tqdm(range(self.n_episodes), desc=f\"Train {self} on {self.env.spec.id}\")\n",
    "        for episode_idx in pbar:\n",
    "\n",
    "            epsilon = np.exp(log_final_eps * episode_idx / self.n_episodes)\n",
    "\n",
    "            state, _ = self.env.reset() # S1\n",
    "            action = self.agent.policy(state, env=self.env, epsilon=epsilon) # A1\n",
    "            terminated = False\n",
    "            episode_length = 1\n",
    "\n",
    "            # play one episode\n",
    "            while not terminated:\n",
    "\n",
    "                next_state, reward, terminated, _, _ = self.env.step(action) # R1, S2\n",
    "                next_action = self.agent.policy(next_state, env=self.env, epsilon=epsilon) # A2\n",
    "\n",
    "                # update the agent\n",
    "                self.agent.update(state, action, reward, next_state, next_action, terminated)\n",
    "\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "                episode_length += 1\n",
    "\n",
    "            self.train_episode_durations.append(episode_length) # store the train episode duration\n",
    "\n",
    "            if eval_every_episode is not None and episode_idx % eval_every_episode == 0:\n",
    "                self.train_episode_indexes.append(episode_idx)\n",
    "                self.eval_episode_durations.append(self.eval(n_episodes=100))\n",
    "                pbar.set_postfix({f\"avg_eval_episode_duration\": np.mean(self.eval_episode_durations[-1])})\n",
    "                \n",
    "        if save_plots:\n",
    "            self.save_train_episode_durations_plot(window=100)\n",
    "            self.save_eval_episode_durations_plot()\n",
    "\n",
    "        if save_agent:\n",
    "            self.agent.save()\n",
    "                \n",
    "        return self.agent\n",
    "\n",
    "\n",
    "# tests\n",
    "\n",
    "mc_trainer = MCTrainer(\n",
    "\tn_episodes=100,\n",
    "\tdiscount_factor=1.0,\n",
    "\tfinal_epsilon=0.1,\n",
    ")\n",
    "print(f\"{mc_trainer} is ready!\")\n",
    "\n",
    "sarsa_lambda_trainer = SARSALambdaTrainer(\n",
    "\tn_episodes=100,\n",
    "\tlearnind_rate=0.01,\n",
    "\ttrace_decay=0.9,\n",
    "\tdiscount_factor=1.0,\n",
    "\tfinal_epsilon=0.1,\n",
    ")\n",
    "print(f\"{sarsa_lambda_trainer} is ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_compare_train_episode_duration(\n",
    "        json_file_1: str,\n",
    "        json_file_2: str,\n",
    "        window: int,\n",
    "        path: str = None,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Compare the training episode durations of two agents whose statistics are stored in json files.\n",
    "    \"\"\" \n",
    "\n",
    "    path = DEFAULT_OUTPUTS_PATH if path is None else path\n",
    "    if not os.path.isfile(os.path.join(path, json_file_1)):\n",
    "        raise FileNotFoundError(f\"File {json_file_1} not found at: {path}\")\n",
    "    if not os.path.isfile(os.path.join(path, json_file_2)):\n",
    "        raise FileNotFoundError(f\"File {json_file_2} not found at: {path}\")\n",
    "    \n",
    "    with open(os.path.join(path, json_file_1), \"r\") as f:\n",
    "        agent1_dict = json.load(f)\n",
    "    \n",
    "    with open(os.path.join(path, json_file_2), \"r\") as f:\n",
    "        agent2_dict = json.load(f)\n",
    "\n",
    "    if len(agent1_dict[\"train_episode_durations\"]) != len(agent2_dict[\"train_episode_durations\"]):\n",
    "        raise ValueError(\"Agents have different number of training episodes...\")\n",
    "    \n",
    "    exp_name_1 = agent1_dict[\"experiment_name\"]\n",
    "    exp_name_2 = agent2_dict[\"experiment_name\"]\n",
    "    train_episodes_1 = np.array(agent1_dict[\"train_episode_durations\"])\n",
    "    train_episodes_2 = np.array(agent2_dict[\"train_episode_durations\"])\n",
    "    \n",
    "    if window is not None and window > len(train_episodes_1):\n",
    "        print(\"Window size should be smaller than the number of training episodes...\")\n",
    "        return\n",
    "    \n",
    "    indexes = np.arange(1, len(train_episodes_1) + 1)\n",
    "    if window is None: # cumulative average\n",
    "        avg_episode_dur_1 = np.cumsum(train_episodes_1) / indexes\n",
    "        avg_episode_dur_2 = np.cumsum(train_episodes_2) / indexes\n",
    "        title_tag = \"(cumulative average)\"\n",
    "    else: # moving average\n",
    "        incomplete_avg_1 = np.cumsum(train_episodes_1[:window-1]) / np.arange(1, window)\n",
    "        complete_avg_1 = np.convolve(train_episodes_1, np.ones(window), 'valid') / window\n",
    "        avg_episode_durations_1 = np.hstack([incomplete_avg_1, complete_avg_1])\n",
    "        incomplete_avg_2 = np.cumsum(train_episodes_2[:window-1]) / np.arange(1, window)\n",
    "        complete_avg_2 = np.convolve(train_episodes_2, np.ones(window), 'valid') / window\n",
    "        avg_episode_durations_2 = np.hstack([incomplete_avg_2, complete_avg_2])\n",
    "        title_tag = f\"(average window = {window})\"\n",
    "\n",
    "    # curve\n",
    "    plt.plot(indexes, avg_episode_durations_1, label=exp_name_1)\n",
    "    plt.plot(indexes, avg_episode_durations_2, label=exp_name_2)\n",
    "\n",
    "    # axis, title and legend\n",
    "    plt.xlabel(\"Training episode index\")\n",
    "    plt.ylabel(f\"Averaged episode duration\")\n",
    "    plt.title(f\"Train episode duration over training {title_tag}\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "\n",
    "    # save plot\n",
    "    plt.savefig(os.path.join(path, f\"DIFF_train_{exp_name_1}_{exp_name_2}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "def plot_and_compare_eval_episode_duration(\n",
    "        json_file_1: str,\n",
    "        json_file_2: str,\n",
    "        window: int,\n",
    "        path: str = None,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Compare the evaluation episode durations of two agents whose statistics are stored in json files.\n",
    "    \"\"\" \n",
    "\n",
    "    path = DEFAULT_OUTPUTS_PATH if path is None else path\n",
    "    if not os.path.isfile(os.path.join(path, json_file_1)):\n",
    "        raise FileNotFoundError(f\"File {json_file_1} not found at: {path}\")\n",
    "    if not os.path.isfile(os.path.join(path, json_file_2)):\n",
    "        raise FileNotFoundError(f\"File {json_file_2} not found at: {path}\")\n",
    "    \n",
    "    with open(os.path.join(path, json_file_1), \"r\") as f:\n",
    "        agent1_dict = json.load(f)\n",
    "    \n",
    "    with open(os.path.join(path, json_file_2), \"r\") as f:\n",
    "        agent2_dict = json.load(f)\n",
    "    \n",
    "    exp_name_1 = agent1_dict[\"experiment_name\"]\n",
    "    exp_name_2 = agent2_dict[\"experiment_name\"]\n",
    "    train_ep_idx_1 = np.array(agent1_dict[\"train_episode_indexes\"])\n",
    "    train_ep_idx_2 = np.array(agent2_dict[\"train_episode_indexes\"])\n",
    "    eval_episodes_1 = np.array(agent1_dict[\"eval_episode_durations\"])\n",
    "    eval_episodes_2 = np.array(agent2_dict[\"eval_episode_durations\"])\n",
    "    max_length_1 = agent1_dict[\"max_episode_length_eval\"]\n",
    "    max_length_2 = agent2_dict[\"max_episode_length_eval\"]  \n",
    "\n",
    "    if len(train_ep_idx_1) == 0 or len(eval_episodes_1) == 0:\n",
    "        raise ValueError(\"No evaluation episode durations to plot for agent 1...\")\n",
    "    \n",
    "    if len(train_ep_idx_2) == 0 or len(eval_episodes_2) == 0:\n",
    "        raise ValueError(\"No evaluation episode durations to plot for agent 2...\")\n",
    "    \n",
    "    if len(train_ep_idx_1) != len(train_ep_idx_2):\n",
    "        raise ValueError(\"Agents have different number of training episodes...\")\n",
    "    \n",
    "    if max_length_1 != max_length_2:\n",
    "        raise ValueError(\"Agents have different maximum episode lengths...\")\n",
    "    \n",
    "    # curve\n",
    "    plt.plot(train_ep_idx_1, [np.mean(lengths) for lengths in eval_episodes_1], label=exp_name_1)\n",
    "    plt.plot(train_ep_idx_2, [np.mean(lengths) for lengths in eval_episodes_2], label=exp_name_2)\n",
    "\n",
    "    # axis, title and legend\n",
    "    plt.xlabel(\"Training episode index\")\n",
    "    plt.ylabel(\"Averaged episode duration\")\n",
    "    plt.title(f\"Evaluation episode duration over training (limit={max_length_1})\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "\n",
    "    # save plot\n",
    "    plt.savefig(os.path.join(path, f\"DIFF_eval_{exp_name_1}_{exp_name_2}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "def plot_epsilon_scheduler(\n",
    "        final_epsilon: float = 0.01, \n",
    "        n_episodes: int = 2000, \n",
    "        path: str = None\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Plot the epsilon decay over the training episodes.\n",
    "    \"\"\"\n",
    "    path = DEFAULT_OUTPUTS_PATH if path is None else path\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    log_final_epsilon = np.log(final_epsilon)\n",
    "    epsilon_list = [np.exp(log_final_epsilon * float(i) / n_episodes) for i in range(n_episodes)]\n",
    "\n",
    "    plt.plot(epsilon_list)\n",
    "    plt.xlabel(\"Training episode index\")\n",
    "    plt.ylabel(\"$\\epsilon$\")\n",
    "    plt.title(\"Exponential epsilon decay over training episodes\")\n",
    "    plt.savefig(os.path.join(path, \"epsilon_scheduler.png\"))\n",
    "    plt.close()\n",
    "\n",
    "def plot_state_value_function(agent_filename: str, path: str = None, save_only: bool = False):\n",
    "    \"\"\"\n",
    "    Plot the state value function of a model stored in a json file.\n",
    "    \"\"\"\n",
    "\n",
    "    agent_filename = agent_filename[:-5] if agent_filename.endswith(\".json\") else agent_filename # remove extension\n",
    "    agent = Agent.from_pretrained(agent_filename, path=path)\n",
    "\n",
    "    min_x, max_x = float('inf'), float('-inf')\n",
    "    min_y, max_y = float('inf'), float('-inf')\n",
    "    for (x,y) in agent.q_values.keys():\n",
    "        min_x = min(min_x, x)\n",
    "        max_x = max(max_x, x)\n",
    "        min_y = min(min_y, y)\n",
    "        max_y = max(max_y, y)\n",
    "\n",
    "    x_range = max_x - min_x + 1\n",
    "    y_range = max_y - min_y + 1\n",
    "    \n",
    "    x_values = np.arange(min_x, max_x + 1, step=1)\n",
    "    y_values = np.arange(min_y, max_y + 1, step=1)\n",
    "\n",
    "    x_mesh, y_mesh = np.meshgrid(x_values, y_values)\n",
    "\n",
    "    q_values = np.zeros((y_range, x_range)) # x_mesh.shape == y_mesh.shape\n",
    "    for (x,y), q in agent.q_values.items():\n",
    "        q_values[int(y-min_y), int(x-min_x)] = np.max(q)\n",
    "\n",
    "\t# 3D plot\n",
    "    fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "    surface = ax.plot_surface(x_mesh, y_mesh, q_values, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "    ax.set_zticks([])\n",
    "    ax.set_xlabel(\"dx\")\n",
    "    ax.set_ylabel(\"dy\")\n",
    "    fig.colorbar(surface, shrink=0.5, aspect=5)\n",
    "    plt.title(f\"State value function of {agent_filename}\")\n",
    "\n",
    "    if not save_only:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(os.path.join(DEFAULT_OUTPUTS_PATH, f\"state_value_function_{agent_filename}.png\"))\n",
    "    \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Launch and visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define environment and training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('TextFlappyBird-v0', height = 15, width = 20, pipe_gap = 4)\n",
    "\n",
    "EPISODES = 2000\n",
    "DISCOUNT_FACTOR = 1.0\n",
    "FINAL_EPSILON = 0.01\n",
    "LEARNING_RATE = 0.1\n",
    "TRACE_DECAY = 0.9\n",
    "\n",
    "EXPERIMENT_NAME = None # if None then a default name defined in each trainer will be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define MC trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = MCTrainer(\n",
    "\tn_episodes=EPISODES,\n",
    "\tdiscount_factor=DISCOUNT_FACTOR,\n",
    "\tfinal_epsilon=FINAL_EPSILON,\n",
    "\tlearning_rate=LEARNING_RATE,\n",
    "\tn_eval=100,\n",
    "\tmax_episode_length_eval=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Sarsa($\\lambda$) trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SARSALambdaTrainer(\n",
    "\tn_episodes=EPISODES,\n",
    "\tlearnind_rate=LEARNING_RATE,\n",
    "\ttrace_decay=TRACE_DECAY,\n",
    "\tdiscount_factor=DISCOUNT_FACTOR,\n",
    "\tfinal_epsilon=FINAL_EPSILON,\n",
    "\tn_eval=100,\n",
    "\tmax_episode_length_eval=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train SARSALambdaTrainer on TextFlappyBird-v0: 100%|██████████| 2000/2000 [00:37<00:00, 54.01it/s, avg_eval_episode_duration=665] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent SARSALambdaAgent saved in /Users/tomlab/Documents/CS/MDS/SM11/RL/assignment/flappybird/models/SARSALambdaAgent.json\n"
     ]
    }
   ],
   "source": [
    "agent = trainer.train(\n",
    "\tenv=env,\n",
    "\texperiment_name=EXPERIMENT_NAME,\n",
    "\tsave_plots=True,\n",
    "\tsave_agent=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the epsilon decay\n",
    "plot_epsilon_scheduler(\n",
    "\tfinal_epsilon=FINAL_EPSILON,\n",
    "\tn_episodes=EPISODES,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the training episode durations of two agents\n",
    "plot_and_compare_train_episode_duration(\n",
    "\tjson_file_1=\"mc_train_durations.json\",\n",
    "\tjson_file_2=\"sarsa-lambda_train_durations.json\",\n",
    "\twindow=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the evaluation episode durations of two agents\n",
    "plot_and_compare_eval_episode_duration(\n",
    "\tjson_file_1=\"mc_eval_durations.json\",\n",
    "\tjson_file_2=\"sarsa-lambda_eval_durations.json\",\n",
    "\twindow=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the state-value functions of the agents\n",
    "plot_state_value_function(agent_filename=\"MCAgent\", save_only=True)\n",
    "plot_state_value_function(agent_filename=\"SARSALambdaAgent\", save_only=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
